# -*- coding: utf-8 -*-
"""AutomationProj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18hNXcxN9eHdXWc5zz8H28SRQs_DXa1Na
"""

import pandas as pd

!pip install chembl_webresource_client

from chembl_webresource_client.new_client import new_client

# data = new_client.target
# data_query = data.search('acetylcholinesterase')

# targets = pd.DataFrame.from_dict(data_query)
# targets.columns

data_new = new_client.activity
data1 = data_new.filter(target_chembl_id='CHEMBL233').filter(standard_type="IC50")

df = pd.DataFrame.from_dict(data1)
df.columns

import matplotlib.pyplot as plt
df.hist()
plt.tight_layout()
plt.show()

import seaborn as sns
cmap=sns.diverging_palette(500,10,as_cmap=True)
sns.heatmap(df.corr(),cmap=cmap,center=0,annot=False,square=True)
plt.show()

def create_categorical(values):
    if isinstance(values[0], list):
        # Convert list to string and then create categorical variable
        return pd.Categorical([str(x) for x in values])
    else:
        # Create categorical variable as usual
        return pd.Categorical(values)

for col in df.columns:
    if df[col].dtype == 'object' and not isinstance(df[col].iloc[0], dict):
        df[col] = create_categorical(df[col])
        df[col] = df[col].cat.codes

cmap=sns.diverging_palette(500,10,as_cmap=True)
sns.heatmap(df.corr(),cmap=cmap,center=0,annot=False,square=True)
plt.show()

print(df['standard_value'].unique())

#dropping records which donot have values in columns standard_value and canonical_smiles
df2 = df[df.standard_value.notna()]
df2 = df2[df2.canonical_smiles.notna()]

#dropping records with duplicate canonical_smiles values to keep them unique
df2_unique = df2.drop_duplicates(['canonical_smiles'])
selection = ['molecule_chembl_id','canonical_smiles','standard_value']
df3 = df2_unique[selection]

print(df['standard_value'].unique())

df2['bioactivity_threshold'] = df2['standard_value'].apply(lambda x: 'active' if float(x) < 1000 else ('inactive' if float(x)  > 10000 else 'intermediate'))
print(df2['bioactivity_threshold'].unique())
sns.countplot(x='bioactivity_threshold', data=df2)

df3.head()

import numpy as np 
df3['plC50']= df3['standard_value'].apply(lambda x: np.log10(float(x)))

df3.head()

!pip install RDKit

from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import rdFMCS

df3['canonical_smiles_mol']= df3['canonical_smiles'].apply(lambda x: AllChem.MolFromSmiles(x))

df3['canonical_smiles_fingerprints']= df3['canonical_smiles_mol'].apply(lambda x: Chem.RDKFingerprint(x))
# fps = [Chem.RDKFingerprint(x) for x in ms]

df3.head()

X= df3['canonical_smiles_fingerprints'].to_numpy()
y= df3['plC50'].to_numpy()

final_data= np.vstack((X, y))

final_data= final_data.T
final_data.shape

final_data= pd.DataFrame(final_data)

final_data.columns

final_data

# import xgboost as xg
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error as MSE

# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
# xgb_r = xg.XGBRegressor(objective ='reg:squarederror',n_estimators = 10, seed = 123)

# xgb_r.fit(X_train, Y_train)
# Y_pred = xgb_r.predict(X_test)

import pandas as pd
import numpy as np
import xgboost as xg
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error as MSE
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
import plotly.graph_objects as go
import random

def ActiveLearn(model, X_training, y_training, X_test, y_test, n_initial, n_queries, querystrat, n_batch): 
  # random.seed()
  # df = df.sample(frac = 1)
  # #split data into X and y 
  # X = df.iloc[0]
  # y = df.iloc[1]

  # X = X.to_numpy()[:, (0,1)]
  # y = y.to_numpy()

  # # get the indicies of the first n observations that were randomly chosen
  # index_initial = np.random.choice(range(len(X)), size=n_initial, replace=False)
  # # save the remaining data points to be sampled from later 
  # remaining = [i for i in range(len(X)) if i not in index_initial]

  # # assign the data points to the training and test arrays 
  # X_training = X[index_initial]
  # y_training= y[index_initial]
  # X_test= X[remaining]
  # y_test = y[remaining]

  #model= xg.XGBRegressor(objective ='reg:squarederror',n_estimators = 10, seed = 123)

  scores=[]

  model.fit(X_training, y_training)

  initial_model_error= cross_val_score(model, X_training, y_training, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
  scores.append(np.abs(initial_model_error))

  unobserved_error=[]
  
  # randomly sample each of the n query points, remove the point from the remianing test data, predict the model performace, then teach it again
  for i in range(int((n_queries-n_initial)/n_batch)):
      
      # query by committee method 
      if querystrat=="random":
        model.fit(X_training, y_training)

        # randomly sample the next 3 points to be added
        indices = np.random.choice(range(len(X_test)), size=3, replace=False)
        queries = [(X_test[i], y_test[i]) for i in indices]

        y_pred = model.predict(X_test)
        scores.append(mean_squared_error(y_test, y_pred))

        model.fit(X_training, y_training)

        # add the points to the training data 
        X_training = np.append(X_training, [X_test[i] for i in indices], axis=0)
        y_training = np.append(y_training, [y_test[i] for i in indices], axis=0)

        # remove the data points we used in training from the remaining test data 
        X_test = np.delete(X_test, indices, 0)  
        y_test = np.delete(y_test, indices, 0)

      # diversity based sampling method 
      elif querystrat== "Diversity Based Sampling": 
        model.fit(X_training, y_training)

        # Fit k-means clustering model to the remaining unobserved test data
        kmeans = KMeans(n_clusters=n_batch).fit(X_test)

        # Get the indices of the centroid closest to each cluster 
        indices = []
        for i in range(n_batch):
          centroid_distances = np.linalg.norm(X_test - kmeans.cluster_centers_[i], axis=1)
          indices.append(np.argmin(centroid_distances))
        
        # Get the indices of the centroid closest to each cluster / random from each cluster
        # indices = []
        # for i in range(n_batch):
        #   cluster_indices = np.where(kmeans.labels_ == i)[0]
        #   random_index = np.random.choice(cluster_indices)
        #   indices.append(random_index)

        queries = [(X_test[i], y_test[i]) for i in indices]

        y_pred = model.predict(X_test)
        scores.append(mean_squared_error(y_test, y_pred))

        model.fit(X_training, y_training)

        # add the points to the training data 
        X_training = np.append(X_training, [X_test[i] for i in indices], axis=0)
        y_training = np.append(y_training, [y_test[i] for i in indices], axis=0)

        # remove the data points we used in training from the remaining test data 
        X_test = np.delete(X_test, indices, 0)  
        y_test = np.delete(y_test, indices, 0)

  y_pred = model.predict(X_test)
  print("last acc score: ", mean_squared_error(y_test, y_pred))
  scores.append(mean_squared_error(y_test, y_pred))
  
  print(scores)
  print("LEN", len(scores))
  return scores

def Simulate(df, n_rounds, n_initial, n_queries):

  total_random=[]
  total_diversity=[]

  for i in range(n_rounds):
    random.seed(i)
    # df= np.concatenate((X, y[:, np.newaxis]), axis=1)
    X = df.iloc[:,0]
    y = df.iloc[:,0]
    X = X.to_numpy()
    y = y.to_numpy()

    # get the indicies of the first n observations that were randomly chosen
    index_initial = np.random.choice(range(len(X)), size=n_initial, replace=False)
    # save the remaining data points to be sampled from later 
    remaining = [i for i in range(len(X)) if i not in index_initial]

    # assign the data points to the training and test arrays 
    X_training = X[index_initial]
    print(X_training)
    y_training= y[index_initial]
    X_test= X[remaining]
    y_test = y[remaining]

    # choose base learner- random forest or svm 
    # model= SVR()
    model= xg.XGBRegressor(objective ='reg:squarederror',n_estimators = 10, seed = 123)
    # model= svm.SVC(probability=True)
    model.fit(X_training, y_training)

    acc_random = ActiveLearn(model, X_training, y_training, X_test, y_test, n_initial, n_queries, "random", 3)
    total_random.append(acc_random)

    acc_diversity = ActiveLearn(model, X_training, y_training, X_test, y_test, n_initial, n_queries, "Diversity Based Sampling", 3)
    total_diversity.append(acc_diversity)

  return total_random, total_diversity

total_random, total_diversity= Simulate(final_data, 1, 10, 1200)